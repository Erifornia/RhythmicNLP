{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLTKaaris",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_htt4lgHpiF",
        "colab_type": "code",
        "outputId": "f924b719-5b69-4172-d94d-08fbd3b3c08f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "source": [
        "!pip install dill\n",
        "!pip install nltk==3.4\n",
        "!pip install epitran"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (0.3.1.1)\n",
            "Collecting nltk==3.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/ed/9c755d357d33bc1931e157f537721efb5b88d2c583fe593cc09603076cc3/nltk-3.4.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.4) (1.12.0)\n",
            "Collecting singledispatch\n",
            "  Downloading https://files.pythonhosted.org/packages/c5/10/369f50bcd4621b263927b0a1519987a04383d4a98fb10438042ad410cf88/singledispatch-3.4.0.3-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4-cp36-none-any.whl size=1436386 sha256=437402ade612cc5980df831777e7c07835f473f423bface0905faa1b8223d070\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/c8/24/b2343664bcceb7147efeb21c0b23703a05b23fcfeaceaa2a1e\n",
            "Successfully built nltk\n",
            "Installing collected packages: singledispatch, nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.4 singledispatch-3.4.0.3\n",
            "Collecting epitran\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/ce/ebf70bb24f220c628d4ce1e153c07e95f59132a1d882b586427ade2b1b3d/epitran-1.8-py2.py3-none-any.whl (132kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hCollecting marisa-trie\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/95/d23071d0992dabcb61c948fb118a90683193befc88c23e745b050a29e7db/marisa-trie-0.7.5.tar.gz (270kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from epitran) (2019.12.20)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from epitran) (46.0.0)\n",
            "Collecting panphon>=0.16\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/25/935f443f0a2cce5d7a4b6b0d9101c990a6f3c74702c02287d4addd3fe009/panphon-0.17-py2.py3-none-any.whl (71kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.6MB/s \n",
            "\u001b[?25hCollecting unicodecsv\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/a4/691ab63b17505a26096608cc309960b5a6bdf39e4ba1a793d5f9b1a53270/unicodecsv-0.14.1.tar.gz\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from panphon>=0.16->epitran) (0.5.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from panphon>=0.16->epitran) (3.13)\n",
            "Collecting munkres\n",
            "  Downloading https://files.pythonhosted.org/packages/64/97/61ddc63578870e04db6eb1d3bee58ad4e727f682068a7c7405edb8b2cdeb/munkres-1.1.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from panphon>=0.16->epitran) (1.18.2)\n",
            "Building wheels for collected packages: marisa-trie, unicodecsv\n",
            "  Building wheel for marisa-trie (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for marisa-trie: filename=marisa_trie-0.7.5-cp36-cp36m-linux_x86_64.whl size=861179 sha256=ec1162cb033887f43acb8d7df53a3a19daea1917f1e42634d06fb3905a1e9082\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/24/79/022624fc914f0e559fe8a1141aaff1f9df810905a13fc75d57\n",
            "  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-cp36-none-any.whl size=10768 sha256=6718e0ad8b51822656dae56daf7388fb8b55a540bea10c15ea53f7ba7a236c55\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/09/e9/e800279c98a0a8c94543f3de6c8a562f60e51363ed26e71283\n",
            "Successfully built marisa-trie unicodecsv\n",
            "Installing collected packages: marisa-trie, munkres, unicodecsv, panphon, epitran\n",
            "Successfully installed epitran-1.8 marisa-trie-0.7.5 munkres-1.1.2 panphon-0.17 unicodecsv-0.14.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwCyroenyltG",
        "colab_type": "code",
        "outputId": "8658944d-e5a7-4531-82eb-922b1f689df0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "from nltk.util import pad_sequence\n",
        "from nltk.util import bigrams\n",
        "from nltk.util import ngrams\n",
        "from nltk.util import everygrams\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "from nltk.lm.preprocessing import flatten\n",
        "from nltk import word_tokenize, sent_tokenize \n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "detokenize = TreebankWordDetokenizer().detokenize\n",
        "from nltk.lm import Laplace\n",
        "\n",
        "import epitran\n",
        "\n",
        "import itertools\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy.random import choice\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "import random\n",
        "from bisect import bisect"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv-dhrWZyr7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "booba = pd.read_csv(\"booba.csv\",index_col=0)\n",
        "damso = pd.read_csv(\"damso.csv\",index_col=0)\n",
        "guizmo = pd.read_csv(\"guizmo.csv\",index_col=0)\n",
        "kaaris = pd.read_csv(\"kaaris.csv\",index_col=0)\n",
        "lomepal = pd.read_csv(\"lomepal.csv\",index_col=0)\n",
        "nekfeu = pd.read_csv(\"nekfeu.csv\",index_col=0)\n",
        "nepal = pd.read_csv(\"nepal.csv\",index_col=0)\n",
        "orelsan = pd.read_csv(\"orelsan.csv\",index_col=0)\n",
        "pnl = pd.read_csv(\"pnl.csv\",index_col=0)\n",
        "sch = pd.read_csv(\"sch.csv\",index_col=0)\n",
        "vald = pd.read_csv(\"vald.csv\",index_col=0)\n",
        "\n",
        "df = booba.append(damso).append(guizmo).append(kaaris).append(lomepal).append(nekfeu).append(nepal).append(orelsan).append(pnl).append(sch).append(vald).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvMyxE4q3XBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initmodel(n=3,df=df):\n",
        "  splilist=[]\n",
        "  for lyr in df.lyrics_clean:\n",
        "    lyr.replace(\"’\",\" \")\n",
        "    lyr.replace(\"“\",\" \")\n",
        "    lyr = lyr.split()\n",
        "    #lyr.insert(0,\"<s>\")\n",
        "    #lyr.append(\"</s>\")\n",
        "    splilist.append(lyr)\n",
        "\n",
        "  train_data_clean = (everygrams(sent, 1, n) for sent in splilist)\n",
        "  sents = flatten(splilist)\n",
        "\n",
        "  model = Laplace(n)\n",
        "  model.fit(train_data_clean, sents)\n",
        "  return model\n",
        "\n",
        "def _random_generator(seed_or_generator):\n",
        "    if isinstance(seed_or_generator, random.Random):\n",
        "        return seed_or_generator\n",
        "    return random.Random(seed_or_generator)\n",
        "\n",
        "def _weighted_choice(population, weights, random_generator=None):\n",
        "    if not population:\n",
        "        raise ValueError(\"Can't choose from empty population\")\n",
        "    if len(population) != len(weights):\n",
        "        raise ValueError(\"The number of weights does not match the population\")\n",
        "    cum_weights = list(itertools.accumulate(weights))\n",
        "    total = cum_weights[-1]\n",
        "    threshold = random_generator.random()\n",
        "    return population[bisect(cum_weights, total * threshold)]\n",
        "\n",
        "def generate(model, num_words=1, text_seed=None, random_seed=None):\n",
        "        text_seed = [] if text_seed is None else list(text_seed)\n",
        "        random_generator = _random_generator(random_seed)\n",
        "        if num_words == 1:\n",
        "            context = (\n",
        "                text_seed[-model.order + 1 :]\n",
        "                if len(text_seed) >= model.order\n",
        "                else text_seed\n",
        "            )\n",
        "            samples = model.context_counts(model.vocab.lookup(context))\n",
        "            while context and not samples:\n",
        "                context = context[1:] if len(context) > 1 else []\n",
        "                samples = model.context_counts(model.vocab.lookup(context))\n",
        "            samples = sorted(samples)\n",
        "            return _weighted_choice(samples, tuple(model.score(w, context) for w in samples), random_generator), samples\n",
        "        generated = []\n",
        "        for _ in range(num_words):\n",
        "            genword, samples = generate(model=model,num_words=1,text_seed=text_seed + generated,random_seed=random_generator)\n",
        "            try:\n",
        "              while genword == generated[-1] and genword == generated[-2]:\n",
        "                genword = random.choice(list(model.vocab))\n",
        "            except IndexError:\n",
        "                pass\n",
        "            generated.append(genword)\n",
        "        return generated\n",
        "\n",
        "def generate_sent(model, num_words, text_seed=None, random_seed=None):\n",
        "    if text_seed is not None:\n",
        "      content = [text_seed]\n",
        "    else:\n",
        "      content = []\n",
        "    for token in generate(model=model, num_words=num_words, text_seed=text_seed, random_seed=random_seed):\n",
        "      if token == \"</s>\":\n",
        "        break\n",
        "      content.append(token)\n",
        "    return \" \".join(content)\n",
        "\n",
        "def dotproduct(v1, v2):\n",
        "  return sum((a*b) for a, b in zip(v1, v2))\n",
        "\n",
        "def length(v):\n",
        "  return math.sqrt(dotproduct(v, v))\n",
        "\n",
        "def angle(v1, v2):\n",
        "  return math.acos(dotproduct(v1, v2) / (length(v1) * length(v2)))\n",
        "\n",
        "def matrixer(sequence):\n",
        "  vowels = [\"j\",\"w\",\"ɥ\",\"a\",\"ɑ\",\"e\",\"ɛ\",\"ɛː\",\"ə\",\"i\",\"œ\",\"ø\",\"o\",\"ɔ\",\"u\",\"y\",\"ɑ̃\",\"ɛ̃\",\"œ̃\",\"ɔ̃\"]\n",
        "  phonemes = [['ɑ','a'],['e', 'ɛ', 'ɛː', 'ə'],['i', 'j'],['o','ɔ'],['wa','wɑ','wɛ̃'],['u','w'],['y','ɥ']\n",
        "              ,['ø','œ','e'],['ɔ̃'], ['ɑ̃'], ['ɛ̃','in','œ̃'], ['b'], ['ks','k','kw'],['sj','si']]\n",
        "  vector = list()\n",
        "  rhyme = list()\n",
        "\n",
        "  epi = epitran.Epitran('fra-Latn')\n",
        "\n",
        "  sequence = sequence.lower()\n",
        "  sequence = epi.transliterate(sequence)\n",
        "\n",
        "  for vowel in vowels:\n",
        "    vector.append(sequence.count(vowel))\n",
        "  \n",
        "  for phoneme in phonemes:\n",
        "    somme = 0\n",
        "    for vowel in phoneme:\n",
        "      somme+=sequence.count(vowel)\n",
        "    vector.append(somme)\n",
        "\n",
        "  seq = ''.join([l for l in sequence if l in vowels])\n",
        "  for vowel in vowels:\n",
        "    if seq[-1] == vowel:\n",
        "      rhyme.append(1)\n",
        "    else:\n",
        "      rhyme.append(0)\n",
        "  return [vector,rhyme]\n",
        "\n",
        "def compare(context,sequences):\n",
        "  sequences.insert(0,context)\n",
        "  vectors = list()\n",
        "  rhymes = list()\n",
        "  angles = list()\n",
        "  \n",
        "  for seq in sequences:\n",
        "    vectors.append(matrixer(seq)[0])\n",
        "    rhymes.append(matrixer(seq)[1])\n",
        "  \n",
        "  for i in range(len(vectors)-1):\n",
        "    if rhymes[0] == rhymes[i+1]:\n",
        "      rhyme = 0\n",
        "    else:\n",
        "      rhyme = 1\n",
        "    angles.append(angle(vectors[0],vectors[i+1])+rhyme)\n",
        "  \n",
        "  print(angles)\n",
        "\n",
        "  return sequences[angles.index(min(angles))+1]\n",
        "\n",
        "def save_model(model,name=\"model.pickle\"):\n",
        "  f = open(name, 'wb')\n",
        "  pickle.dump(model, f)\n",
        "  f.close()\n",
        "\n",
        "def load_model(model,name=\"model.pickle\"):\n",
        "  f = open(name, 'rb')\n",
        "  model = pickle.load(f)\n",
        "  f.close()\n",
        "  return model\n",
        "\n",
        "def couplet(model=initmodel(),sents=5,comp=5):\n",
        "  sentlist = [generate_sent(model,np.random.randint(5,21))]\n",
        "  for i in range(sents):\n",
        "    context = sentlist[-1]\n",
        "    trials=list()\n",
        "    for j in range(comp):\n",
        "      trials.append(generate_sent(model,np.random.randint(5,21)))\n",
        "      print(\"Trial\",j,\"done\")\n",
        "    sentlist.append(compare(context, trials))\n",
        "    print(\"Sentence\",i,\"done\")\n",
        "  return \"\\n\".join(sentlist)\n",
        "\n",
        "def refrain(model=initmodel(), comp=5, form=None):\n",
        "  sentlist = [\" \".join(model.generate(np.random.randint(5,15)))]\n",
        "  context = sentlist[0]\n",
        "  trials = list()\n",
        "  for j in range(comp):\n",
        "    trials.append(\" \".join(model.generate(np.random.randint(5,15))))\n",
        "  sentlist.append(compare(context,trials))\n",
        "  if form == None:\n",
        "    form = np.random.randint(0,3)\n",
        "  if form == 0:\n",
        "    return \"\\n\".join([sentlist[0],sentlist[1],sentlist[0],sentlist[1]])\n",
        "  elif form == 1:\n",
        "    return \"\\n\".join([sentlist[0],sentlist[0],sentlist[1],sentlist[1]])\n",
        "  elif form == 2:\n",
        "    return \"\\n\".join([sentlist[0],sentlist[1],sentlist[1],sentlist[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "cab2d11c-3d9d-4055-da19-6edc2fa7b098",
        "id": "1ng2QnNeDfZZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "print(couplet(initmodel(3,pnl)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 0 done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-26ba887703ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcouplet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpnl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-38158dda5f40>\u001b[0m in \u001b[0;36mcouplet\u001b[0;34m(model, sents, comp)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m       \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trial\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0msentlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-38158dda5f40>\u001b[0m in \u001b[0;36mgenerate_sent\u001b[0;34m(model, num_words, text_seed, random_seed)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"</s>\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-38158dda5f40>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model, num_words, text_seed, random_seed)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mgenword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_seed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m               \u001b[0;32mwhile\u001b[0m \u001b[0mgenword\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgenword\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-38158dda5f40>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model, num_words, text_seed, random_seed)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_weighted_choice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-38158dda5f40>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_weighted_choice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/lm/api.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, word, context)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \"\"\"\n\u001b[1;32m    138\u001b[0m         return self.unmasked_score(\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         )\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/lm/models.py\u001b[0m in \u001b[0;36munmasked_score\u001b[0;34m(self, word, context)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mword_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mnorm_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword_count\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnorm_count\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/lm/vocabulary.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;34m\"\"\"Computing size of vocabulary reflects the cutoff.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/lm/vocabulary.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;34m\"\"\"Computing size of vocabulary reflects the cutoff.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6gP_Jlp-_cv",
        "colab_type": "code",
        "outputId": "fbed39d1-2d2c-4e8d-8354-9515122b58a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(refrain(initmodel(3,pnl)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.9520676361226457, 1.7771624968888613, 1.8851494618969569, 1.7505598546399697, 1.527896747552734]\n",
            "faut filoche le q7 ganda les\n",
            "j maille et puis ne s ouvriront qu à peser les grammes bon\n",
            "faut filoche le q7 ganda les\n",
            "j maille et puis ne s ouvriront qu à peser les grammes bon\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-V4A_D-kTbOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}